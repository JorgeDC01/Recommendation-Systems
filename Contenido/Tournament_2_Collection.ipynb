{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20d5891b",
   "metadata": {},
   "source": [
    "# **SISTEMAS DE RECOMENDACIÓN**\n",
    "\n",
    "## **Filtrado Basado en Contenido**\n",
    "\n",
    "\n",
    "Miembros del Grupo:\n",
    "- Paula Arias Fernández\n",
    "- Jorge del Castillo Gómez\n",
    "- Anny Álvarez Nogales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2edb182",
   "metadata": {},
   "outputs": [],
   "source": [
    "### BORRAR ESTE COMENTARIO CUANDO LO LEÁIS\n",
    "#Sigo la estructura de la presentación \n",
    "# (dejo los huequitos antes de mi código pero si queréis cambiar \n",
    "# el orden de algo o moverlo sin problema que no sabia como ponerlo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f07809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa142c74",
   "metadata": {},
   "source": [
    "# **Pruebas Datos Textuales** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22c45af",
   "metadata": {},
   "source": [
    "## Pruebas iniciales\n",
    "\n",
    "1. TF-IDF:\n",
    "    - TFIDF + LogisticRegression -  `MAE: 0.65`\n",
    "    - TFIDF + RandomForestRegression - `MAE: 0.82`\n",
    "    - TFIDF + xgboost - `MAE: - 0.65`\n",
    "2. Doc2Vec + LogisticRegression - `MAE: 1.24`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c4bdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Tfidf elimina las stopwords\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=200, ngram_range=(1, 2))\n",
    "\n",
    "X_train_tfidf = vectorizer.fit_transform(df_train['text'])\n",
    "X_test_tfidf = vectorizer.transform(df_test['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a0935d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_train['stars']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_tfidf, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f'Mean Absolute Error (Train Test Split): {mae}')\n",
    "\n",
    "y_pred_test = model.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f1288a",
   "metadata": {},
   "source": [
    "**GridSearch** con LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24c54b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_train['stars']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_tfidf, y, test_size=0.2, random_state=42)\n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],     # Regularización\n",
    "    'solver': ['liblinear', 'saga'],  # Métodos de optimización\n",
    "    'max_iter': [100, 500, 1000],     # Número máximo de iteraciones\n",
    "}\n",
    "\n",
    "model = LogisticRegression(random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='neg_mean_absolute_error', n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "best_params = grid_search.best_params_\n",
    "y_pred_best = best_model.predict(X_test)\n",
    "mae_best = mean_absolute_error(y_test, y_pred_best)\n",
    "\n",
    "print(f\"Mejores parámetros: {best_params}\")\n",
    "print(f\"Mean Absolute Error (mejor modelo): {mae_best}\")\n",
    "\n",
    "# Comparacion entre el mejor modelo y uno predeterminado (sin optimizar)\n",
    "model_default = LogisticRegression(max_iter=1000, random_state=42)\n",
    "model_default.fit(X_train, y_train)\n",
    "y_pred_default = model_default.predict(X_test)\n",
    "mae_default = mean_absolute_error(y_test, y_pred_default)\n",
    "print(f\"Mean Absolute Error (modelo sin optimizar): {mae_default}\")\n",
    "\n",
    "### Predicciones\n",
    "y_pred_test = best_model.predict(X_test_tfidf)\n",
    "\n",
    "submission_df = pd.DataFrame({\n",
    "    'review_id': df_test['review_id'],  \n",
    "    'stars': y_pred_test  \n",
    "})\n",
    "\n",
    "submission_df.to_csv('prediction_tfidf_logisticReg_gridSearch.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86372cec",
   "metadata": {},
   "source": [
    "**Con un Clasificador RandomForest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efabcf90",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=100, ngram_range=(1, 2))\n",
    "\n",
    "X_train_tfidf = vectorizer.fit_transform(df_train['text'])\n",
    "X_test_tfidf = vectorizer.transform(df_test['text'])\n",
    "\n",
    "### Con RandomForest\n",
    "y = df_train['stars']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_tfidf, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = RandomForestRegressor(n_estimators=10, random_state=42, n_jobs=-1)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f'Mean Absolute Error (Train Test Split): {mae}')\n",
    "\n",
    "y_pred_test = model.predict(X_test_tfidf)\n",
    "\n",
    "submission_df = pd.DataFrame({\n",
    "    'review_id': df_test['review_id'],\n",
    "    'stars': y_pred_test\n",
    "})\n",
    "\n",
    "submission_df.to_csv('prediction_tfidf_randomForest.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae9a501",
   "metadata": {},
   "source": [
    "**Con un clasificador xgboost**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28e253f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "y = df_train['stars'].astype(int) - 1\n",
    "\n",
    "model = XGBClassifier(n_estimators=100, use_label_encoder=False, eval_metric='mlogloss', random_state=42, n_jobs=-1)\n",
    "model.fit(X_train_tfidf, y)\n",
    "\n",
    "y_pred_test = model.predict(X_test_tfidf) + 1\n",
    "\n",
    "submission_df = pd.DataFrame({\n",
    "    'review_id': df_test['review_id'],\n",
    "    'stars': y_pred_test\n",
    "})\n",
    "\n",
    "submission_df.to_csv('prediction_tfidf_xgboost.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3a8c81",
   "metadata": {},
   "source": [
    "### **Doc2Vec**\n",
    "Enlace: https://spotintelligence.com/2023/09/06/doc2vec/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503a75c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "#### PREPROCESADO ####\n",
    "######################\n",
    "print(\"--- Inicio Preprocesado...\")\n",
    "\n",
    "# Tokenizer\n",
    "def preprocess_text_parallel(text):\n",
    "    return re.findall(r'\\b[a-zA-Z]+\\b', text.lower())\n",
    "\n",
    "df_train['tokens'] = Parallel(n_jobs=-1)(delayed(preprocess_text_parallel)(text) for text in df_train['text'])\n",
    "df_test['tokens'] = Parallel(n_jobs=-1)(delayed(preprocess_text_parallel)(text) for text in df_test['text'])\n",
    "\n",
    "# TaggedDocument\n",
    "tagged_train = [TaggedDocument(words=tokens, tags=[str(i)]) for i, tokens in enumerate(df_train['tokens'])]\n",
    "\n",
    "########################\n",
    "#### MODELO DOC2VEC ####\n",
    "########################\n",
    "\n",
    "# Initialize the Doc2Vec model\n",
    "model = Doc2Vec(vector_size=50,   # Dimensionality of the document vectors\n",
    "                window=2,         # Maximum distance between the current and predicted word within a sentence\n",
    "                min_count=1,      # Ignores all words with total frequency lower than this\n",
    "                workers=-1,       # Number of CPU cores to use for training\n",
    "                epochs=2)         # Number of training epochs\n",
    "\n",
    "model.build_vocab(tagged_train)\n",
    "model.train(tagged_train, total_examples=len(tagged_train), epochs=model.epochs)\n",
    "\n",
    "# Inferir vectores\n",
    "df_train['vector'] = df_train['tokens'].apply(lambda x: model.infer_vector(x))\n",
    "df_test['vector'] = df_test['tokens'].apply(lambda x: model.infer_vector(x))\n",
    "\n",
    "\n",
    "X_test = list(df_test['vector'])\n",
    "\n",
    "#############################\n",
    "#### MODELO CLASIFICADOR ####\n",
    "#############################\n",
    "\n",
    "X_list_train = list(df_train['vector'])\n",
    "y_train = df_train['stars']\n",
    "\n",
    "\n",
    "classifier = LogisticRegression(max_iter=1000)\n",
    "classifier.fit(X_list_train, y_train)\n",
    "\n",
    "predicted_stars = classifier.predict(X_test)\n",
    "\n",
    "submission_df = pd.DataFrame({\n",
    "    'review_id': df_test['review_id'],\n",
    "    'stars': predicted_stars\n",
    "})\n",
    "submission_df.to_csv('prediction_doc2vec_logreg.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97de994",
   "metadata": {},
   "source": [
    "## Word2Vec Embedding Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bd4ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATOS\n",
    "train_reviews=pd.read_csv('train_reviews.csv', sep=',')\n",
    "test_reviews = pd.read_csv('test_reviews.csv')\n",
    "negocios_df=pd.read_csv('negocios.csv')\n",
    "\n",
    "df = train_reviews.merge(negocios_df, on='business_id',how='inner')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a526937",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PREPROCESS DATA \n",
    "\n",
    "df['text'] = df['text'].astype(str) + \" \" + df['categories'].astype(str)\n",
    "\n",
    "def preprocess_text_parallel(text):\n",
    "    return re.findall(r'\\b[a-zA-Z]+\\b', text.lower())\n",
    "\n",
    "train_reviews['tokens'] = Parallel(n_jobs=-1)(delayed(preprocess_text_parallel)(text) for text in df['text'])\n",
    "test_reviews['tokens'] = Parallel(n_jobs=-1)(delayed(preprocess_text_parallel)(text) for text in test_reviews['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6294ac70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEXT MODEL WORD2VEC\n",
    "#se entrena una vez y se guarda el modelo\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(\n",
    "    sentences=train_reviews['tokens'],\n",
    "    vector_size=50,  \n",
    "    window=5,\n",
    "    min_count=5,  \n",
    "    workers=4,  \n",
    "    epochs=7 \n",
    ")\n",
    "\n",
    "\n",
    "#model.save(\"word2vec_model.model\")\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec.load(\"word2vec_model.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d112e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#APPLY TEXT EMBEDDING MODEL\n",
    "def get_review_vector(tokens, model):\n",
    "    vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
    "    if len(vectors) == 0:\n",
    "        return [0] * model.vector_size  \n",
    "    return sum(vectors) / len(vectors)\n",
    "\n",
    "train_reviews['vector'] = train_reviews['tokens'].apply(lambda x: get_review_vector(x, model))\n",
    "\n",
    "X_train = list(train_reviews['vector'])\n",
    "y_train = train_reviews['stars']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946c78b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLASSIFIER LOGISTIC REGRESSION\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression(max_iter=2500,C= 14.867708330182724,solver='newton-cg',penalty='l2')\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "test_reviews['vector'] = test_reviews['tokens'].apply(lambda x: get_review_vector(x, model))\n",
    "\n",
    "X_test = list(test_reviews['vector'])\n",
    "predicted_stars = classifier.predict(X_test)\n",
    "\n",
    "submission_df = pd.DataFrame({\n",
    "    'review_id': test_reviews['review_id'],\n",
    "    'stars': predicted_stars\n",
    "})\n",
    "\n",
    "submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2d44d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLASSIFIER RANDOMFOREST\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "test_reviews['vector'] = test_reviews['tokens'].apply(lambda x: get_review_vector(x, model))\n",
    "\n",
    "X_test = list(test_reviews['vector'])\n",
    "predicted_stars = classifier.predict(X_test)\n",
    "\n",
    "submission_df = pd.DataFrame({\n",
    "    'review_id': test_reviews['review_id'],\n",
    "    'stars': predicted_stars\n",
    "})\n",
    "\n",
    "\n",
    "submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a62f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGBOOST CLASSIFIER\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "y_train_adj = y_train - 1\n",
    "\n",
    "classifier = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)\n",
    "classifier.fit(X_train, y_train_adj)\n",
    "\n",
    "X_test = list(test_reviews['vector'])\n",
    "predicted_stars = classifier.predict(X_test) + 1  \n",
    "\n",
    "\n",
    "submission_df = pd.DataFrame({\n",
    "    'review_id': test_reviews['review_id'],\n",
    "    'stars': predicted_stars\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69630a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OPTUNA WITH LOGISTIC REGRESSION\n",
    "import optuna\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Optimización para el mejor modelo de regresión logística\n",
    "\n",
    "def objective(trial):\n",
    "    C = trial.suggest_loguniform('C', 1e-5, 100)  \n",
    "    solver = trial.suggest_categorical('solver', ['liblinear', 'lbfgs', 'newton-cg', 'saga'])\n",
    "    max_iter = trial.suggest_int('max_iter', 1000, 5000, step=500)\n",
    "    penalty = trial.suggest_categorical('penalty', ['l2'])\n",
    "\n",
    "    classifier = LogisticRegression(C=C, solver=solver, max_iter=max_iter, penalty=penalty)\n",
    "    classifier.fit(X_train, y_train)\n",
    "\n",
    "    predicted_stars = classifier.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, predicted_stars)\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Maximize accuracy\n",
    "study = optuna.create_study(direction='maximize')  \n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "best_params = study.best_params\n",
    "print(f\"Best hyperparameters: {best_params}\")\n",
    "\n",
    "best_classifier = LogisticRegression(**best_params)\n",
    "best_classifier.fit(X_train, y_train)\n",
    "\n",
    "#evaluation\n",
    "final_predictions = best_classifier.predict(X_test)\n",
    "final_accuracy = accuracy_score(y_test, final_predictions)\n",
    "print(f\"Final Accuracy: {final_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8185d9cf",
   "metadata": {},
   "source": [
    "## Fast Text Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b495d55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEXT MODEL FAST TEXT\n",
    "from gensim.models import FastText\n",
    "\n",
    "\n",
    "fasttext_model = FastText(sentences=train_reviews['tokens'], vector_size=50, window=5, min_count=5, epochs=10)\n",
    "fasttext_model.save(\"fasttext_model.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb5ab46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_review_vector(tokens, model):\n",
    "    vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
    "    if len(vectors) == 0:\n",
    "        return [0] * model.vector_size\n",
    "    \n",
    "    return sum(vectors) / len(vectors)\n",
    "\n",
    "train_reviews['vector'] = train_reviews['tokens'].apply(lambda x: get_review_vector(x, fasttext_model))\n",
    "\n",
    "X_train = list(train_reviews['vector'])\n",
    "y_train = train_reviews['stars']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cf7640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBOOST CLASSIFIER\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "y_train_adj = y_train - 1\n",
    "\n",
    "classifier = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)\n",
    "classifier.fit(X_train, y_train_adj)\n",
    "\n",
    "X_test = list(test_reviews['vector'])\n",
    "predicted_stars = classifier.predict(X_test) + 1  \n",
    "\n",
    "\n",
    "submission_df = pd.DataFrame({\n",
    "    'review_id': test_reviews['review_id'],\n",
    "    'stars': predicted_stars\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d08e13",
   "metadata": {},
   "source": [
    "# **Pruebas Datos No Textuales**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99164546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURES SELECTION\n",
    "usuarios=pd.read_csv('usuarios.csv', sep=',')\n",
    "\n",
    "df = df.merge(usuarios, on='user_id',how='inner')\n",
    "\n",
    "df2=df.drop(['review_id',\t'user_id'\t,'business_id','text', 'date', 'name_x','address','city','state','attributes','categories','name_y','elite','friends','hours','yelping_since','postal_code'],axis=1)\n",
    "df2.rename(columns={'stars_x':'stars','name_y':'user_name','useful_x':'useful','funny_x':'funny','cool_x':'cool'}, inplace=True)\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3b8dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NO TEXTUAL DATA NORMALIZATION\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "df2=df2[['funny','stars','cool','useful']]\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "\n",
    "y_train = df2['stars'].round().astype(int)  \n",
    "X_train = df2.drop(columns=['stars'])  \n",
    "\n",
    "\n",
    "# Normalización\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Entrenamiento\n",
    "classifier = LogisticRegression(max_iter=3000)\n",
    "classifier.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084d9bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLASSIFIER\n",
    "\n",
    "#X_test_scaled = scaler.transform(test_reviews)\n",
    "common_cols = list(set(X_train.columns) & set(test_reviews.columns))\n",
    "X_test = test_reviews[common_cols]\n",
    "X_test_scaled = scaler.fit_transform(X_test)\n",
    "\n",
    "X_test_scaled\n",
    "predicted_stars = classifier.predict(X_test_scaled)\n",
    "predicted_stars\n",
    "\n",
    "submission_df = pd.DataFrame({\n",
    "    'review_id': test_reviews['review_id'],\n",
    "    'stars': predicted_stars\n",
    "})\n",
    "\n",
    "submission_df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639a917a",
   "metadata": {},
   "source": [
    "# **Modelo TwoTower y ThreeTower**\n",
    "\n",
    "El modelo Two-Tower (o modelo de dos torres) es una arquitectura de red neuronal que aprende representaciones (embeddings) separadas para diferentes tipos de entrada —en este caso, usuarios y textos de reseñas—, y luego las combina para predecir una puntuación.\n",
    "\n",
    "**Arquitectura**\n",
    "\n",
    "1. User Tower:\n",
    "\n",
    "    - Recibe el user_id como entrada.\n",
    "\n",
    "    - Aplica una capa de embedding para convertir el ID del usuario en un vector denso.\n",
    "\n",
    "    - Pasa ese vector por una capa totalmente conectada (Linear) con activación ReLU.\n",
    "\n",
    "    - Su objetivo es aprender una representación del usuario basada en su historial.\n",
    "\n",
    "2. Review Tower:\n",
    "\n",
    "    - Recibe la representación vectorial de la reseña (TF-IDF).\n",
    "\n",
    "    - Procesa esta entrada mediante una capa lineal con ReLU.\n",
    "\n",
    "    - Aprender una representación semántica del texto.\n",
    "\n",
    "3. Head:\n",
    "\n",
    "    - Concatena los vectores generados por ambas torres.\n",
    "\n",
    "    - Pasa el vector combinado por una red neuronal profunda:\n",
    "\n",
    "    - Capa Linear + Dropout + ReLU.\n",
    "\n",
    "    - Capa final Linear que produce un único valor escalar (la predicción del rating)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169ac6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YelpReviewDataset(Dataset):\n",
    "    def __init__(self, df, df_users, text_vectorizer):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.df_users = df_users.set_index(\"user_id\")\n",
    "        self.vectorizer = text_vectorizer\n",
    "\n",
    "        # Label Encoding de user_id\n",
    "        self.user_encoder = LabelEncoder()\n",
    "        self.df[\"user_id_encoded\"] = self.user_encoder.fit_transform(self.df[\"user_id\"])\n",
    "\n",
    "        # Emparejamos info de usuarios\n",
    "        #self.df[\"review_text_vec\"] = list(self.vectorizer.transform(self.df[\"text\"]).toarray())\n",
    "        self.review_matrix = self.vectorizer.transform(self.df[\"text\"])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        user_id = row[\"user_id_encoded\"]\n",
    "        review_vec_sparse = self.review_matrix[idx]\n",
    "        review_vec_dense = torch.tensor(review_vec_sparse.toarray(), dtype=torch.float32).squeeze(0)\n",
    "        rating = torch.tensor(row[\"stars\"], dtype=torch.float32)\n",
    "\n",
    "        return user_id, review_vec_dense, rating\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class YelpReviewTestDataset(Dataset):\n",
    "    def __init__(self, df, df_users, text_vectorizer):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.df_users = df_users.set_index(\"user_id\")\n",
    "        self.vectorizer = text_vectorizer\n",
    "\n",
    "        # Label Encoding de user_id\n",
    "        self.user_encoder = LabelEncoder()\n",
    "        self.df[\"user_id_encoded\"] = self.user_encoder.fit_transform(self.df[\"user_id\"])\n",
    "\n",
    "        # Emparejamos info de usuarios\n",
    "        #self.df[\"review_text_vec\"] = list(self.vectorizer.transform(self.df[\"text\"]).toarray())\n",
    "        self.review_matrix = self.vectorizer.transform(self.df[\"text\"])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        user_id = row[\"user_id_encoded\"]\n",
    "        review_vec_sparse = self.review_matrix[idx]\n",
    "        review_vec_dense = torch.tensor(review_vec_sparse.toarray(), dtype=torch.float32).squeeze(0)\n",
    "\n",
    "        return user_id, review_vec_dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee20a2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preentrena TF-IDF sobre el corpus completo\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "vectorizer.fit(df_train[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e41e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserTower(nn.Module):\n",
    "    def __init__(self, num_users, embedding_dim=64, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.user_embedding = nn.Embedding(num_users, embedding_dim)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, user_id):\n",
    "        x = self.user_embedding(user_id)\n",
    "        return self.fc(x)\n",
    "\n",
    "class ReviewTower(nn.Module):\n",
    "    def __init__(self, input_dim=300, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, review_vec):\n",
    "        return self.fc(review_vec)\n",
    "\n",
    "class TwoTowerModel(nn.Module):\n",
    "    def __init__(self, num_users, text_input_dim=300, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.user_tower = UserTower(num_users, hidden_dim=hidden_dim)\n",
    "        self.review_tower = ReviewTower(text_input_dim, hidden_dim=hidden_dim)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, 256),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, user_id, review_vec):\n",
    "        user_embed = self.user_tower(user_id)\n",
    "        review_embed = self.review_tower(review_vec)\n",
    "        combined = torch.cat([user_embed, review_embed], dim=-1)\n",
    "        return self.head(combined).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b927a924",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "###################################\n",
    "############## DATOS ##############\n",
    "###################################\n",
    "train_df, val_df = train_test_split(df_train, test_size=0.1, random_state=42)\n",
    "\n",
    "train_dataset = YelpReviewDataset(train_df, df_users, vectorizer)\n",
    "val_dataset = YelpReviewDataset(val_df, df_users, vectorizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=256)\n",
    "val_loader = DataLoader(val_dataset, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f32895",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################\n",
    "############## MODELO ####################\n",
    "##########################################\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TwoTowerModel(num_users=len(train_dataset.user_encoder.classes_), text_input_dim=5000).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-3)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "print(\"--- 1/2 Entrenamiento ---\")\n",
    "for epoch in range(1):\n",
    "    epoch_time = time.time()\n",
    "\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for i, (user_ids, review_vecs, stars) in enumerate(train_loader):\n",
    "\n",
    "        user_ids = user_ids.to(device)\n",
    "        review_vecs = review_vecs.to(device)\n",
    "        stars = stars.to(device)\n",
    "\n",
    "        preds = model(user_ids, review_vecs)\n",
    "        loss = criterion(preds, stars)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    epoch_end_time = time.time()\n",
    "    epoch_time = epoch_end_time - epoch_time\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_loader):.4f}, Time: {epoch_time:.2f}s\")\n",
    "\n",
    "print(\"\")\n",
    "print(\"--- Validación ---\")\n",
    "model.eval()\n",
    "total_loss = 0\n",
    "with torch.no_grad():\n",
    "    for i, (user_ids, review_vecs, stars) in enumerate(val_loader):\n",
    "        user_ids = user_ids.to(device)\n",
    "        review_vecs = review_vecs.to(device)\n",
    "        stars = stars.to(device)\n",
    "\n",
    "        preds = model(user_ids, review_vecs)\n",
    "        loss = criterion(preds, stars)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "print(f\"Validation Loss: {total_loss / len(val_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021b2eab",
   "metadata": {},
   "source": [
    "> Continuación del entrenamiento con los datos de validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e977e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- 2/2 Entrenamiento ---\")\n",
    "for epoch in range(1):\n",
    "    epoch_time = time.time()\n",
    "\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for i, (user_ids, review_vecs, stars) in enumerate(val_loader):\n",
    "\n",
    "        user_ids = user_ids.to(device)\n",
    "        review_vecs = review_vecs.to(device)\n",
    "        stars = stars.to(device)\n",
    "\n",
    "        preds = model(user_ids, review_vecs)\n",
    "        loss = criterion(preds, stars)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    epoch_end_time = time.time()\n",
    "    epoch_time = epoch_end_time - epoch_time\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss / len(val_loader):.4f}, Time: {epoch_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f83bd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "# # Elimina referencias explícitamente\n",
    "# del train_df, val_df, train_dataset, val_dataset, train_loader, val_loader\n",
    "\n",
    "# # Limpia el recolector de basura de Python\n",
    "# gc.collect()\n",
    "\n",
    "\n",
    "# review_id, stars\n",
    "test_dataset = YelpReviewTestDataset(df_test, df_users, vectorizer)\n",
    "test_loader = DataLoader(test_dataset, batch_size=256)\n",
    "\n",
    "# Test set\n",
    "model.eval()\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for i, (user_ids, review_vecs) in enumerate(test_loader):\n",
    "        user_ids = user_ids.to(device)\n",
    "        review_vecs = review_vecs.to(device)\n",
    "        preds = model(user_ids, review_vecs)\n",
    "        predictions.extend(preds.cpu().numpy())\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Batch {i}/{len(test_loader)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e352b2",
   "metadata": {},
   "source": [
    "> Prueba con TF-IDF de 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b18a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############\n",
    "### TF-IDF ###\n",
    "##############\n",
    "vectorizer = TfidfVectorizer(max_features=20000)\n",
    "vectorizer.fit(df_train[\"text\"])\n",
    "\n",
    "import time\n",
    "\n",
    "###################################\n",
    "############## DATOS ##############\n",
    "###################################\n",
    "train_df, val_df = train_test_split(df_train, test_size=0.1, random_state=42)\n",
    "\n",
    "train_dataset = YelpReviewDataset(train_df, df_users, vectorizer)\n",
    "val_dataset = YelpReviewDataset(val_df, df_users, vectorizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=256)\n",
    "val_loader = DataLoader(val_dataset, batch_size=256)\n",
    "\n",
    "#######################################################\n",
    "############## ENTRENAMIENTO Y VALIDACIÓN #############\n",
    "#######################################################\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TwoTowerModel(num_users=len(train_dataset.user_encoder.classes_), text_input_dim=20000).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=4e-3)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "print(\"--- 1/2 Entrenamiento ---\")\n",
    "for epoch in range(1):\n",
    "    epoch_time = time.time()\n",
    "\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for i, (user_ids, review_vecs, stars) in enumerate(train_loader):\n",
    "\n",
    "        user_ids = user_ids.to(device)\n",
    "        review_vecs = review_vecs.to(device)\n",
    "        stars = stars.to(device)\n",
    "\n",
    "        preds = model(user_ids, review_vecs)\n",
    "        loss = criterion(preds, stars)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    epoch_end_time = time.time()\n",
    "    epoch_time = epoch_end_time - epoch_time\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_loader):.4f}, Time: {epoch_time:.2f}s\")\n",
    "\n",
    "print(\"\")\n",
    "print(\"--- Validación ---\")\n",
    "model.eval()\n",
    "total_loss = 0\n",
    "with torch.no_grad():\n",
    "    for i, (user_ids, review_vecs, stars) in enumerate(val_loader):\n",
    "        user_ids = user_ids.to(device)\n",
    "        review_vecs = review_vecs.to(device)\n",
    "        stars = stars.to(device)\n",
    "\n",
    "        preds = model(user_ids, review_vecs)\n",
    "        loss = criterion(preds, stars)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "print(f\"Validation Loss: {total_loss / len(val_loader):.4f}\")\n",
    "\n",
    "\n",
    "print(\"--- 2/2 Entrenamiento ---\")\n",
    "for epoch in range(1):\n",
    "    epoch_time = time.time()\n",
    "\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for i, (user_ids, review_vecs, stars) in enumerate(val_loader):\n",
    "\n",
    "        user_ids = user_ids.to(device)\n",
    "        review_vecs = review_vecs.to(device)\n",
    "        stars = stars.to(device)\n",
    "\n",
    "        preds = model(user_ids, review_vecs)\n",
    "        loss = criterion(preds, stars)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    epoch_end_time = time.time()\n",
    "    epoch_time = epoch_end_time - epoch_time\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss / len(val_loader):.4f}, Time: {epoch_time:.2f}s\")\n",
    "\n",
    "\n",
    "##########################################\n",
    "############## PREDICCIONES ##############\n",
    "##########################################\n",
    "\n",
    "test_dataset = YelpReviewTestDataset(df_test, df_users, vectorizer)\n",
    "test_loader = DataLoader(test_dataset, batch_size=256)\n",
    "\n",
    "model.eval()\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for i, (user_ids, review_vecs) in enumerate(test_loader):\n",
    "        user_ids = user_ids.to(device)\n",
    "        review_vecs = review_vecs.to(device)\n",
    "        preds = model(user_ids, review_vecs)\n",
    "        predictions.extend(preds.cpu().numpy())\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Batch {i}/{len(test_loader)}\")\n",
    "\n",
    "submission_df = pd.DataFrame({\n",
    "    'review_id': df_test['review_id'],\n",
    "    'stars': predictions\n",
    "})\n",
    "submission_df.to_csv('prediction_TwoTowers_8.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9743cb5c",
   "metadata": {},
   "source": [
    "### Segundo Experimento. TF-IDF + ThreeTower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef16315a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YelpReviewDataset(Dataset):\n",
    "    def __init__(self, df, df_users, df_businesses, text_vectorizer):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.df_users = df_users.set_index(\"user_id\")\n",
    "        self.df_businesses = df_businesses.set_index(\"business_id\")\n",
    "        self.vectorizer = text_vectorizer\n",
    "\n",
    "        # Label Encoding de user_id y business_id\n",
    "        self.user_encoder = LabelEncoder()\n",
    "        self.business_encoder = LabelEncoder()\n",
    "\n",
    "        self.df[\"user_id_encoded\"] = self.user_encoder.fit_transform(self.df[\"user_id\"])\n",
    "        self.df[\"business_id_encoded\"] = self.business_encoder.fit_transform(self.df[\"business_id\"])\n",
    "\n",
    "        # Vectorizamos el texto\n",
    "        self.review_matrix = self.vectorizer.transform(self.df[\"text\"])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        user_id = row[\"user_id_encoded\"]\n",
    "        business_id = row[\"business_id_encoded\"]\n",
    "\n",
    "        review_vec_sparse = self.review_matrix[idx]\n",
    "        review_vec_dense = torch.tensor(review_vec_sparse.toarray(), dtype=torch.float32).squeeze(0)\n",
    "\n",
    "        rating = torch.tensor(row[\"stars\"], dtype=torch.float32)\n",
    "\n",
    "        return user_id, business_id, review_vec_dense, rating\n",
    "\n",
    "\n",
    "class YelpReviewTestDataset(Dataset):\n",
    "    def __init__(self, df, df_users, df_businesses, text_vectorizer):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.df_users = df_users.set_index(\"user_id\")\n",
    "        self.df_businesses = df_businesses.set_index(\"business_id\")\n",
    "        self.vectorizer = text_vectorizer\n",
    "\n",
    "        # Label Encoding de user_id y business_id\n",
    "        self.user_encoder = LabelEncoder()\n",
    "        self.business_encoder = LabelEncoder()\n",
    "\n",
    "        self.df[\"user_id_encoded\"] = self.user_encoder.fit_transform(self.df[\"user_id\"])\n",
    "        self.df[\"business_id_encoded\"] = self.business_encoder.fit_transform(self.df[\"business_id\"])\n",
    "\n",
    "        # Vectorizamos el texto\n",
    "        self.review_matrix = self.vectorizer.transform(self.df[\"text\"])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        user_id = row[\"user_id_encoded\"]\n",
    "        business_id = row[\"business_id_encoded\"]\n",
    "\n",
    "        review_vec_sparse = self.review_matrix[idx]\n",
    "        review_vec_dense = torch.tensor(review_vec_sparse.toarray(), dtype=torch.float32).squeeze(0)\n",
    "\n",
    "        # En el dataset de test no necesitamos el rating, por eso no lo devolvemos\n",
    "        return user_id, business_id, review_vec_dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1291a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preentrena TF-IDF sobre el corpus completo\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "vectorizer.fit(df_train[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d33f77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserTower(nn.Module):\n",
    "    def __init__(self, num_users, embedding_dim=64, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.user_embedding = nn.Embedding(num_users, embedding_dim)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, user_id):\n",
    "        x = self.user_embedding(user_id)\n",
    "        return self.fc(x)\n",
    "\n",
    "class BusinessTower(nn.Module):\n",
    "    def __init__(self, num_businesses, embedding_dim=64, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.business_embedding = nn.Embedding(num_businesses, embedding_dim)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, business_id):\n",
    "        x = self.business_embedding(business_id)\n",
    "        return self.fc(x)\n",
    "\n",
    "class ReviewTower(nn.Module):\n",
    "    def __init__(self, input_dim=300, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, review_vec):\n",
    "        return self.fc(review_vec)\n",
    "\n",
    "class ThreeTowerModel(nn.Module):\n",
    "    def __init__(self, num_users, num_businesses, text_input_dim=300, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        # Aseguramos que las dimensiones sean correctas para cada torre\n",
    "        self.user_tower = UserTower(num_users, embedding_dim=64, hidden_dim=hidden_dim)\n",
    "        self.business_tower = BusinessTower(num_businesses, embedding_dim=64, hidden_dim=hidden_dim)\n",
    "        self.review_tower = ReviewTower(input_dim=text_input_dim, hidden_dim=hidden_dim)\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 3, 256),  # Multiplicamos por 3, ya que concatenamos user, business y review\n",
    "            nn.Dropout(0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, user_id, business_id, review_vec):\n",
    "        # Hacemos que las tres torres devuelvan embeddings compatibles\n",
    "        user_embed = self.user_tower(user_id)\n",
    "        business_embed = self.business_tower(business_id)\n",
    "        review_embed = self.review_tower(review_vec)\n",
    "\n",
    "        # Concatenamos las representaciones de las tres torres\n",
    "        combined = torch.cat([user_embed, business_embed, review_embed], dim=-1)\n",
    "\n",
    "        # Pasa la concatenación por la cabeza para hacer la predicción final\n",
    "        return self.head(combined).squeeze(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2194622e",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "############## DATOS ##############\n",
    "###################################\n",
    "\n",
    "# Split de entrenamiento/validación\n",
    "train_df, val_df = train_test_split(df_train, test_size=0.1, random_state=42)\n",
    "\n",
    "train_dataset = YelpReviewDataset(train_df, df_users, df_businesses, vectorizer)\n",
    "val_dataset = YelpReviewDataset(val_df, df_users, df_businesses, vectorizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=256)\n",
    "val_loader = DataLoader(val_dataset, batch_size=256)\n",
    "\n",
    "#######################################################\n",
    "############## ENTRENAMIENTO Y VALIDACIÓN #############\n",
    "#######################################################\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a744a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajusta el modelo para que reciba tanto el user_id como el business_id\n",
    "model = ThreeTowerModel(\n",
    "    num_users=len(train_dataset.user_encoder.classes_),  # Número de usuarios\n",
    "    num_businesses=len(train_dataset.business_encoder.classes_),  # Número de negocios (tienes que definir un encoder para 'business_id')\n",
    "    text_input_dim=5000  # Ajusta según la dimensión de tu entrada de texto\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=4e-3)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "print(\"--- 1/2 Entrenamiento ---\")\n",
    "for epoch in range(1):\n",
    "    epoch_time = time.time()\n",
    "\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for i, (user_ids, business_ids, review_vecs, stars) in enumerate(train_loader):\n",
    "\n",
    "        user_ids = user_ids.to(device)\n",
    "        business_ids = business_ids.to(device)  # Ahora también tomamos business_id\n",
    "        review_vecs = review_vecs.to(device)\n",
    "        stars = stars.to(device)\n",
    "\n",
    "        preds = model(user_ids, business_ids, review_vecs)\n",
    "        loss = criterion(preds, stars)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    epoch_end_time = time.time()\n",
    "    epoch_time = epoch_end_time - epoch_time\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_loader):.4f}, Time: {epoch_time:.2f}s\")\n",
    "\n",
    "print(\"\")\n",
    "print(\"--- Validación ---\")\n",
    "model.eval()\n",
    "total_loss = 0\n",
    "with torch.no_grad():\n",
    "    for i, (user_ids, business_ids, review_vecs, stars) in enumerate(val_loader):\n",
    "        user_ids = user_ids.to(device)\n",
    "        business_ids = business_ids.to(device)  # Ahora también tomamos business_id\n",
    "        review_vecs = review_vecs.to(device)\n",
    "        stars = stars.to(device)\n",
    "\n",
    "        preds = model(user_ids, business_ids, review_vecs)\n",
    "        loss = criterion(preds, stars)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "print(f\"Validation Loss: {total_loss / len(val_loader):.4f}\")\n",
    "\n",
    "print(\"--- 2/2 Entrenamiento ---\")\n",
    "for epoch in range(1):\n",
    "    epoch_time = time.time()\n",
    "\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for i, (user_ids, business_ids, review_vecs, stars) in enumerate(val_loader):\n",
    "\n",
    "        user_ids = user_ids.to(device)\n",
    "        business_ids = business_ids.to(device)  # Ahora también tomamos business_id\n",
    "        review_vecs = review_vecs.to(device)\n",
    "        stars = stars.to(device)\n",
    "\n",
    "        preds = model(user_ids, business_ids, review_vecs)\n",
    "        loss = criterion(preds, stars)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    epoch_end_time = time.time()\n",
    "    epoch_time = epoch_end_time - epoch_time\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss / len(val_loader):.4f}, Time: {epoch_time:.2f}s\")\n",
    "\n",
    "##########################################\n",
    "############## PREDICCIONES ##############\n",
    "##########################################\n",
    "\n",
    "# Dataset de test con business_id también\n",
    "test_dataset = YelpReviewTestDataset(df_test, df_users, df_businesses, vectorizer)\n",
    "test_loader = DataLoader(test_dataset, batch_size=256)\n",
    "\n",
    "model.eval()\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for i, (user_ids, business_ids, review_vecs) in enumerate(test_loader):\n",
    "        user_ids = user_ids.to(device)\n",
    "        business_ids = business_ids.to(device)  # Ahora también tomamos business_id\n",
    "        review_vecs = review_vecs.to(device)\n",
    "        preds = model(user_ids, business_ids, review_vecs)\n",
    "        predictions.extend(preds.cpu().numpy())\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Batch {i}/{len(test_loader)}\")\n",
    "\n",
    "# Guardar las predicciones en el archivo de salida\n",
    "submission_df = pd.DataFrame({\n",
    "    'review_id': df_test['review_id'],\n",
    "    'stars': predictions\n",
    "})\n",
    "submission_df.to_csv('prediction_TwoTowers_with_business.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
