{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7pXAbayjuSqc"
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "import random\n",
    "from scipy.special import digamma\n",
    "from math import exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install scikit-surprise --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mejor solución actual**:\n",
    "\n",
    "- SVD: Mejores parámetros: {'n_factors': 50, 'n_epochs': 30, 'lr_all': 0.005, 'reg_all': 0.06}. Predicciones guardadas en\n",
    "predictions\\svd_gridsearch\\predictions_svd.csv. MAE: 1.260\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reader Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_train = pd.read_csv('train.csv', sep=',', index_col=False)\n",
    "df_test = pd.read_csv('test.csv', sep=',', index_col=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>25715</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>25716</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>25851</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>25923</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>25924</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user   item  rating\n",
       "0     1  25715     7.0\n",
       "1     1  25716    10.0\n",
       "2     5  25851     9.0\n",
       "3     6  25923     5.0\n",
       "4     7  25924     6.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import operator\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "import sys\n",
    "from surprise import Dataset, Reader\n",
    "from surprise import KNNBasic, SVD\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise import accuracy\n",
    "from surprise.dataset import DatasetAutoFolds\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'surprise.dataset.DatasetAutoFolds'>\n"
     ]
    }
   ],
   "source": [
    "reader = Reader(rating_scale=(0,10)) # rating scale range\n",
    "data = Dataset.load_from_df(df_train[['user', 'item', 'rating']], reader)\n",
    "print(type(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'surprise.trainset.Trainset'>\n"
     ]
    }
   ],
   "source": [
    "trainset, testset = train_test_split(data, test_size=0.25)\n",
    "print(type(trainset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = data.build_full_trainset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<surprise.prediction_algorithms.matrix_factorization.SVD at 0x1fecbe35b20>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algo = SVD()\n",
    "algo.fit(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = algo.test(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:  0.7635\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7634623190271725"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy.mae(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Exportacion de predicciones.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3891, 81714, 10.0),\n",
       " (49435, 77613, 8.0),\n",
       " (38654, 112871, 6.0),\n",
       " (69560, 55505, 10.0),\n",
       " (69799, 28638, 9.0)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "solution = []\n",
    "\n",
    "for _, row in df_test.iterrows():\n",
    "    user = row['user']\n",
    "    item = row['item']\n",
    "    \n",
    "    pred = algo.predict(user, item).est  # Predicción de rating\n",
    "    solution.append([row['ID'], pred])\n",
    "\n",
    "solution_df = pd.DataFrame(solution, columns=[\"ID\", \"rating\"])\n",
    "\n",
    "solution_df.to_csv('predictions_svd.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Pruebas SVD ajustando hiperparametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed: 10.6min\n",
      "[Parallel(n_jobs=-1)]: Done 243 out of 243 | elapsed: 15.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores parámetros: {'n_factors': 50, 'n_epochs': 30, 'lr_all': 0.005, 'reg_all': 0.06}\n",
      "Predicciones guardadas en svd_gridsearch\\predictions_svd.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from surprise import Dataset, Reader, SVD\n",
    "from surprise.model_selection import GridSearchCV\n",
    "\n",
    "output_dir = \"predictions/svd_gridsearch\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Configurar el rango de ratings\n",
    "reader = Reader(rating_scale=(df_train[\"rating\"].min(), df_train[\"rating\"].max()))\n",
    "data = Dataset.load_from_df(df_train[['user', 'item', 'rating']], reader)\n",
    "\n",
    "param_grid = {\n",
    "    \"n_factors\": [50, 100, 150],     # Aumentar complejidad\n",
    "    \"n_epochs\": [20, 30, 50],        # Más iteraciones para mejor convergencia\n",
    "    \"lr_all\": [0.002, 0.005, 0.01],  # Ajuste de tasa de aprendizaje\n",
    "    \"reg_all\": [0.02, 0.04, 0.06],   # Regularización para evitar overfitting\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(SVD, param_grid, measures=[\"mae\"], cv=3, n_jobs=-1, joblib_verbose=1)\n",
    "grid_search.fit(data)\n",
    "\n",
    "best_params = grid_search.best_params[\"mae\"]\n",
    "print(f\"Mejores parámetros: {best_params}\")\n",
    "\n",
    "### ENTRENO  EL MEJOR MODELO CON TODOS LOS DATOS\n",
    "best_svd = SVD(**best_params, random_state=42)\n",
    "trainset = data.build_full_trainset()\n",
    "best_svd.fit(trainset)\n",
    "\n",
    "solution = []\n",
    "for _, row in df_test.iterrows():\n",
    "    user = row[\"user\"]\n",
    "    item = row[\"item\"]\n",
    "    pred = best_svd.predict(user, item).est\n",
    "    solution.append([row[\"ID\"], pred])\n",
    "\n",
    "# Guardar predicciones en CSV\n",
    "output_file = os.path.join(output_dir, \"predictions_svd.csv\")\n",
    "solution_df = pd.DataFrame(solution, columns=[\"ID\", \"rating\"])\n",
    "solution_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Predicciones guardadas en {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = data.build_full_trainset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing epoch 0\n",
      "Processing epoch 1\n",
      "Processing epoch 2\n",
      "Processing epoch 3\n",
      "Processing epoch 4\n",
      "Processing epoch 5\n",
      "Processing epoch 6\n",
      "Processing epoch 7\n",
      "Processing epoch 8\n",
      "Processing epoch 9\n",
      "Processing epoch 10\n",
      "Processing epoch 11\n",
      "Processing epoch 12\n",
      "Processing epoch 13\n",
      "Processing epoch 14\n",
      "Processing epoch 15\n",
      "Processing epoch 16\n",
      "Processing epoch 17\n",
      "Processing epoch 18\n",
      "Processing epoch 19\n",
      "Processing epoch 20\n",
      "Processing epoch 21\n",
      "Processing epoch 22\n",
      "Processing epoch 23\n",
      "Processing epoch 24\n",
      "Processing epoch 25\n",
      "Processing epoch 26\n",
      "Processing epoch 27\n",
      "Processing epoch 28\n",
      "Processing epoch 29\n",
      "Processing epoch 30\n",
      "Processing epoch 31\n",
      "Processing epoch 32\n",
      "Processing epoch 33\n",
      "Processing epoch 34\n",
      "Processing epoch 35\n",
      "Processing epoch 36\n",
      "Processing epoch 37\n",
      "Processing epoch 38\n",
      "Processing epoch 39\n",
      "Processing epoch 40\n",
      "Processing epoch 41\n",
      "Processing epoch 42\n",
      "Processing epoch 43\n",
      "Processing epoch 44\n",
      "Processing epoch 45\n",
      "Processing epoch 46\n",
      "Processing epoch 47\n",
      "Processing epoch 48\n",
      "Processing epoch 49\n",
      "MAE:  1.7760\n"
     ]
    }
   ],
   "source": [
    "from surprise import Dataset, Reader, NMF\n",
    "\n",
    "algo = NMF(n_factors=15, n_epochs=50, biased=False, reg_pu=0.06, reg_qi=0.06, random_state=42, verbose=True)\n",
    "algo.fit(trainset)\n",
    "predictions = algo.test(testset)\n",
    "\n",
    "accuracy.mae(predictions)\n",
    "\n",
    "solution = []\n",
    "\n",
    "for _, row in df_test.iterrows():\n",
    "    user = row['user']\n",
    "    item = row['item']\n",
    "    \n",
    "    pred = algo.predict(user, item).est  # Predicción de rating\n",
    "    solution.append([row['ID'], pred])\n",
    "\n",
    "solution_df = pd.DataFrame(solution, columns=[\"ID\", \"rating\"])\n",
    "\n",
    "solution_df.to_csv('predictions_nmf.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing epoch 0\n",
      "Processing epoch 1\n",
      "Processing epoch 2\n",
      "Processing epoch 3\n",
      "Processing epoch 4\n",
      "Processing epoch 5\n",
      "Processing epoch 6\n",
      "Processing epoch 7\n",
      "Processing epoch 8\n",
      "Processing epoch 9\n",
      "Processing epoch 10\n",
      "Processing epoch 11\n",
      "Processing epoch 12\n",
      "Processing epoch 13\n",
      "Processing epoch 14\n",
      "Processing epoch 15\n",
      "Processing epoch 16\n",
      "Processing epoch 17\n",
      "Processing epoch 18\n",
      "Processing epoch 19\n",
      "Processing epoch 20\n",
      "Processing epoch 21\n",
      "Processing epoch 22\n",
      "Processing epoch 23\n",
      "Processing epoch 24\n",
      "Processing epoch 25\n",
      "Processing epoch 26\n",
      "Processing epoch 27\n",
      "Processing epoch 28\n",
      "Processing epoch 29\n",
      "Processing epoch 30\n",
      "Processing epoch 31\n",
      "Processing epoch 32\n",
      "Processing epoch 33\n",
      "Processing epoch 34\n",
      "Processing epoch 35\n",
      "Processing epoch 36\n",
      "Processing epoch 37\n",
      "Processing epoch 38\n",
      "Processing epoch 39\n",
      "Processing epoch 40\n",
      "Processing epoch 41\n",
      "Processing epoch 42\n",
      "Processing epoch 43\n",
      "Processing epoch 44\n",
      "Processing epoch 45\n",
      "Processing epoch 46\n",
      "Processing epoch 47\n",
      "Processing epoch 48\n",
      "Processing epoch 49\n",
      "Processing epoch 50\n",
      "Processing epoch 51\n",
      "Processing epoch 52\n",
      "Processing epoch 53\n",
      "Processing epoch 54\n",
      "Processing epoch 55\n",
      "Processing epoch 56\n",
      "Processing epoch 57\n",
      "Processing epoch 58\n",
      "Processing epoch 59\n",
      "Processing epoch 60\n",
      "Processing epoch 61\n",
      "Processing epoch 62\n",
      "Processing epoch 63\n",
      "Processing epoch 64\n",
      "Processing epoch 65\n",
      "Processing epoch 66\n",
      "Processing epoch 67\n",
      "Processing epoch 68\n",
      "Processing epoch 69\n",
      "Processing epoch 70\n",
      "Processing epoch 71\n",
      "Processing epoch 72\n",
      "Processing epoch 73\n",
      "Processing epoch 74\n"
     ]
    }
   ],
   "source": [
    "from surprise import Dataset, Reader, NMF\n",
    "\n",
    "algo = NMF(n_factors=40, n_epochs=75, biased=False, reg_pu=0.06, reg_qi=0.06, random_state=42, verbose=True)\n",
    "algo.fit(trainset)\n",
    "predictions = algo.test(testset)\n",
    "\n",
    "# accuracy.mae(predictions)\n",
    "\n",
    "solution = []\n",
    "\n",
    "for _, row in df_test.iterrows():\n",
    "    user = row['user']\n",
    "    item = row['item']\n",
    "    \n",
    "    pred = algo.predict(user, item).est  # Predicción de rating\n",
    "    solution.append([row['ID'], pred])\n",
    "\n",
    "solution_df = pd.DataFrame(solution, columns=[\"ID\", \"rating\"])\n",
    "\n",
    "solution_df.to_csv('predictions_nmf_k40_e75.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GridSearch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from surprise import Dataset, Reader, NMF\n",
    "from surprise.model_selection import GridSearchCV\n",
    "\n",
    "# Crear el directorio donde se guardarán los archivos\n",
    "output_dir = \"nmf_gridsearch\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Cargar datos\n",
    "df_train = pd.read_csv(\"train.csv\")  # Reemplaza con tu archivo real\n",
    "df_test = pd.read_csv(\"test.csv\")  # Reemplaza con tu archivo real\n",
    "\n",
    "# Configurar el rango de ratings\n",
    "reader = Reader(rating_scale=(df_train[\"rating\"].min(), df_train[\"rating\"].max()))\n",
    "data = Dataset.load_from_df(df_train[['user', 'item', 'rating']], reader)\n",
    "\n",
    "# Definir la malla de hiperparámetros para NMF\n",
    "param_grid = {\n",
    "    \"n_factors\": [10, 15, 20],\n",
    "    \"n_epochs\": [30, 50, 70],\n",
    "    \"reg_pu\": [0.04, 0.06, 0.08],\n",
    "    \"reg_qi\": [0.04, 0.06, 0.08],\n",
    "}\n",
    "\n",
    "# Ejecutar Grid Search\n",
    "grid_search = GridSearchCV(NMF, param_grid, measures=[\"mae\"], cv=3, n_jobs=-1)\n",
    "grid_search.fit(data)\n",
    "\n",
    "# Obtener los mejores parámetros\n",
    "best_params = grid_search.best_params[\"mae\"]\n",
    "print(f\"Mejores parámetros: {best_params}\")\n",
    "\n",
    "# Entrenar el mejor modelo con todos los datos\n",
    "best_nmf = NMF(**best_params, random_state=42)\n",
    "trainset = data.build_full_trainset()\n",
    "best_nmf.fit(trainset)\n",
    "\n",
    "# Generar predicciones para df_test\n",
    "solution = []\n",
    "for _, row in df_test.iterrows():\n",
    "    user = row[\"user\"]\n",
    "    item = row[\"item\"]\n",
    "    pred = best_nmf.predict(user, item).est\n",
    "    solution.append([row[\"ID\"], pred])\n",
    "\n",
    "# Guardar predicciones en CSV\n",
    "solution_df = pd.DataFrame(solution, columns=[\"ID\", \"rating\"])\n",
    "output_file = os.path.join(output_dir, \"predictions_nmf.csv\")\n",
    "solution_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Predicciones guardadas en {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "3.4. BNMF.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
